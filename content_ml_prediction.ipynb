{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import statistics\n",
    "from readability import Readability\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "pipeline = joblib.load(\"./data/knn_pipeline.joblib\")\n",
    "\n",
    "# features to be used \n",
    "# verbs_third_person, verbs_others, words_per_sentence_median, num_of_sentences, adverbs_rate, nouns_rate, \n",
    "# adjectives_rate, verbs_third_person_rate, verbs_others_rate, automatic_readability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string into data point\n",
    "# run prediction\n",
    "class Content_Predictor:\n",
    "    def __init__(self, pipeline) -> None:\n",
    "        self.pipeline: Pipeline = pipeline\n",
    "        self.adverbs = [\"RB\", \"RBR\", \"RBS\", \"WRB\"]\n",
    "        self.nouns = [\"NN\", \"NNS\"]\n",
    "        self.proper_nouns = [\"NNP\", \"NNPS\"]\n",
    "        self.adjective = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "        self.conjunctions = [\"CC\"]\n",
    "        self.verbs_third_person = [\"VB\", \"VBD\", \"VBG\", \"VBN\"]\n",
    "        self.verbs_others = [\"VBP\", \"VBZ\"]\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "    def __combine_columns(self, row):\n",
    "        return str(row[\"title\"]) + \". \" + str(row[\"text\"])\n",
    "\n",
    "    def process_readability(self, text_iterator):\n",
    "        readability_ls = []\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove non ascii (language english only)\n",
    "                t = re.sub(r\"[^\\x00-\\x7F]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                if len(t) == 0:\n",
    "                    readability_ls.append(np.NaN)\n",
    "                else:\n",
    "                    r = Readability(t)\n",
    "                    readability_ls.append(r.ari().score)\n",
    "            except:\n",
    "                readability_ls.append(np.NaN)\n",
    "                # print(text)\n",
    "                continue\n",
    "        print(\"Complete process ari\")\n",
    "        return readability_ls\n",
    "\n",
    "    def process_content_sentences(self, text_iterator):\n",
    "        sentence_median_length = []\n",
    "        num_of_sentences = []\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove some supers\n",
    "                t = re.sub(\"[^a-zA-Z0-9\\.\\?\\!]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                # tokenise sentences\n",
    "                t = re.split(\"\\!|\\.|\\?\", t)\n",
    "                # strip sentence\n",
    "                t = [item.strip() for item in t]\n",
    "                # drop empty string\n",
    "                t = list(filter(lambda x: x != \"\", t))\n",
    "                # tokenise words\n",
    "                sentences_tokenised = []\n",
    "                sentences_lengths = []\n",
    "                for sentence in t:\n",
    "                    # tokenise\n",
    "                    s = sentence.split(\" \")\n",
    "                    sentences_lengths.append(len(s))\n",
    "                    sentences_tokenised.append(s)\n",
    "                # sentence features\n",
    "                sentence_median_length.append(statistics.median(sentences_lengths))\n",
    "                num_of_sentences.append(len(sentences_tokenised))\n",
    "                # print(sentences_tokenised)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                sentence_median_length.append(0)\n",
    "                num_of_sentences.append(0)\n",
    "                continue\n",
    "        print(\"Complete process sentences\")\n",
    "        return sentence_median_length, num_of_sentences\n",
    "\n",
    "    def __count_word_pos(self, doc, word_type):\n",
    "        count = 0\n",
    "        for word in doc:\n",
    "            if word[1] in word_type:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def process_pos(self, text_iterator):\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text_processed = []  # no stop words, as pos tokens\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove punctuation\n",
    "                t = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                # tokenise\n",
    "                t = t.split(\" \")\n",
    "                # drop empty string\n",
    "                t = list(filter(lambda x: x != \"\", t))\n",
    "                no_stop_words = []\n",
    "                for token in t:\n",
    "                    if token not in stop_words:\n",
    "                        no_stop_words.append(token)\n",
    "                try:\n",
    "                    text_processed.append(pos_tag(no_stop_words))\n",
    "                except:\n",
    "                    text_processed.append([])\n",
    "            except:\n",
    "                text_processed.append(None)\n",
    "                continue\n",
    "        return text_processed\n",
    "\n",
    "    def predict(self, drop_label=False):\n",
    "        data_input = self.data.dropna()\n",
    "        if drop_label:\n",
    "            data = data_input.drop(columns=\"label\").values\n",
    "        else:\n",
    "            data = data_input.values\n",
    "        prediction = self.pipeline.predict(data)\n",
    "        return prediction\n",
    "\n",
    "    def score(self):\n",
    "        # check for label data column\n",
    "        print(list(self.data.columns))       \n",
    "        data_input = self.data.dropna()\n",
    "        score = self.pipeline.score(\n",
    "            data_input.drop(columns=\"label\").values, data_input[\"label\"].values\n",
    "        )\n",
    "        return score\n",
    "\n",
    "    def __load_data_inner(self, dirty_merged_data):\n",
    "        self.data[\"words_per_sentence_median\"], self.data[\"num_of_sentences\"] = (\n",
    "            self.process_content_sentences(dirty_merged_data)\n",
    "        )\n",
    "        pos_tokens = self.process_pos(dirty_merged_data)\n",
    "        self.data[\"adverbs\"] = [\n",
    "            self.__count_word_pos(doc, self.adverbs) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"nouns\"] = [\n",
    "            self.__count_word_pos(doc, self.nouns) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"adjectives\"] = [\n",
    "            self.__count_word_pos(doc, self.adjective) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"verbs_third_person\"] = [\n",
    "            self.__count_word_pos(doc, self.verbs_third_person) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"verbs_others\"] = [\n",
    "            self.__count_word_pos(doc, self.verbs_others) for doc in pos_tokens\n",
    "        ]\n",
    "\n",
    "        self.data[\"adverbs_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"adverbs\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"nouns_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"nouns\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"adjectives_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"adjectives\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"verbs_third_person_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"verbs_third_person\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"verbs_others_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"verbs_others\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        # print(self.data.shape)\n",
    "        self.data = self.data.drop(columns=[\"adverbs\", \"nouns\", \"adjectives\"])\n",
    "        # print(self.data.shape)\n",
    "        return\n",
    "\n",
    "    def load_content_data(self, data: pd.DataFrame) -> None:\n",
    "        # we take in the dataframe of untouched csv\n",
    "        self.data[\"label\"] = data[\"label\"]\n",
    "        dirty_merged_data = data.apply(self.__combine_columns, axis=1)\n",
    "        self.__load_data_inner(dirty_merged_data)\n",
    "        # reorganise the table format.\n",
    "        self.data = self.data.reindex(\n",
    "            [\n",
    "                \"label\",\n",
    "                \"verbs_third_person\",\n",
    "                \"verbs_others\",\n",
    "                \"words_per_sentence_median\",\n",
    "                \"num_of_sentences\",\n",
    "                \"adverbs_rate\",\n",
    "                \"nouns_rate\",\n",
    "                \"adjectives_rate\",\n",
    "                \"verbs_third_person_rate\",\n",
    "                \"verbs_others_rate\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        print(\"finished load_content_data()\")\n",
    "        return\n",
    "\n",
    "    def load_readability_data(self, data: pd.DataFrame):\n",
    "        if (len(self.data.columns)) == 0:\n",
    "            print(\"running load_content_data first\")\n",
    "            self.load_content_data(data)            \n",
    "        dirty_merged_data = data.apply(self.__combine_columns, axis=1)        \n",
    "        readability_ls = self.process_readability(dirty_merged_data)\n",
    "        self.data[\"ari\"] = readability_ls\n",
    "        print(\"finish load_readability_data\")\n",
    "        return\n",
    "\n",
    "    def load_content_string(self, string: str) -> None:\n",
    "        self.data = pd.DataFrame()\n",
    "        self.__load_data_inner(string)\n",
    "        print(\"finish load_content_string\")\n",
    "        # reorganise the table format.\n",
    "        self.data = self.data.reindex(\n",
    "            [\n",
    "                \"verbs_third_person\",\n",
    "                \"verbs_others\",\n",
    "                \"words_per_sentence_median\",\n",
    "                \"num_of_sentences\",\n",
    "                \"adverbs_rate\",\n",
    "                \"nouns_rate\",\n",
    "                \"adjectives_rate\",\n",
    "                \"verbs_third_person_rate\",\n",
    "                \"verbs_others_rate\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        return \n",
    "    \n",
    "    def load_readability_string(self, string: str) -> None:\n",
    "        print(\"running load_content_data first\")\n",
    "        self.load_content_string(string)\n",
    "        readability_ls = self.process_readability(string)\n",
    "        # print(self.data.shape)\n",
    "        # print(readability_ls)\n",
    "        self.data[\"ari\"] = readability_ls\n",
    "        print(\"finish load_readability_string\")\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete process sentences\n",
      "finished load_content_data()\n"
     ]
    }
   ],
   "source": [
    "predictor = Content_Predictor(pipeline)\n",
    "data = pd.read_csv(\"./data/WELFake_Dataset.csv\")\n",
    "data = data.sample(frac=0.01)\n",
    "test = [\"The Francis Scott Key Bridge was a steel arch continuous through truss bridge that spanned the lower Patapsco River and outer Baltimore Harbor / Port carrying the Baltimore Beltway (Interstate 695 or I-695) between Hawkins Point, an isolated southern neighborhood of Baltimore, and Dundalk in Maryland, United States. The crossing between Baltimore City and Baltimore County also passed through a small portion of Anne Arundel County. The main spans and part of the northeastern approach of the bridge collapsed on March 26, 2024 after the container ship MV Dali struck one of its piers. n the 1960s, the Maryland State Roads Commission concluded a need for a second harbor crossing after the earlier Baltimore Harbor Thruway and Tunnel opened in 1957. They began planning another single-tube tunnel under the Patapsco River, further to the southeast, downstream from the Baltimore Harbor Tunnel. The proposed site was between Hawkins Point and Sollers Point in the outer harbor. Plans also were under way for a drawbridge to the south over Curtis Creek, replacing an earlier 1931 drawbridge carrying Pennington Avenue over the creek, to connect Hawkins Point to Sollers Point.\"]\n",
    "# test = [\"this is a test. um oops! not really? haha haha!\",\"thats the rub. whats a rub?? rubber# duck duck!\"]\n",
    "# predictor.process_content_sentences(test)\n",
    "# print(predictor.process_pos(test))\n",
    "# predictor.process_readability(test)\n",
    "predictor.load_content_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 10)\n",
      "['label', 'verbs_third_person', 'verbs_others', 'words_per_sentence_median', 'num_of_sentences', 'adverbs_rate', 'nouns_rate', 'adjectives_rate', 'verbs_third_person_rate', 'verbs_others_rate']\n",
      "0.9431345353675451\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17635</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>28.5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32078</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>1.485714</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>67</td>\n",
       "      <td>16.5</td>\n",
       "      <td>80</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3.987500</td>\n",
       "      <td>1.975000</td>\n",
       "      <td>1.387500</td>\n",
       "      <td>0.837500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>23.5</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.772727</td>\n",
       "      <td>2.454545</td>\n",
       "      <td>2.136364</td>\n",
       "      <td>0.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>3.729730</td>\n",
       "      <td>1.716216</td>\n",
       "      <td>1.337838</td>\n",
       "      <td>0.554054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "17635      1                  26            16                       28.5   \n",
       "32078      1                  52            25                       18.0   \n",
       "1510       1                 111            67                       16.5   \n",
       "5272       1                  47            18                       23.5   \n",
       "26132      1                  99            41                       12.0   \n",
       "\n",
       "       num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "17635                12      0.750000    9.333333         4.000000   \n",
       "32078                35      0.485714    6.200000         2.571429   \n",
       "1510                 80      0.525000    3.987500         1.975000   \n",
       "5272                 22      1.000000    5.772727         2.454545   \n",
       "26132                74      0.540541    3.729730         1.716216   \n",
       "\n",
       "       verbs_third_person_rate  verbs_others_rate  \n",
       "17635                 2.166667           1.333333  \n",
       "32078                 1.485714           0.714286  \n",
       "1510                  1.387500           0.837500  \n",
       "5272                  2.136364           0.818182  \n",
       "26132                 1.337838           0.554054  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictor.data.shape)\n",
    "print(predictor.score())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete process ari\n",
      "finish load_readability_data\n"
     ]
    }
   ],
   "source": [
    "predictor.load_readability_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(721, 11)\n",
      "['label', 'verbs_third_person', 'verbs_others', 'words_per_sentence_median', 'num_of_sentences', 'adverbs_rate', 'nouns_rate', 'adjectives_rate', 'verbs_third_person_rate', 'verbs_others_rate', 'ari']\n",
      "0.957613814756672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17635</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>28.5</td>\n",
       "      <td>12</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>24.406113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32078</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>2.571429</td>\n",
       "      <td>1.485714</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>20.586790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>1</td>\n",
       "      <td>111</td>\n",
       "      <td>67</td>\n",
       "      <td>16.5</td>\n",
       "      <td>80</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>3.987500</td>\n",
       "      <td>1.975000</td>\n",
       "      <td>1.387500</td>\n",
       "      <td>0.837500</td>\n",
       "      <td>11.442837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5272</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>23.5</td>\n",
       "      <td>22</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.772727</td>\n",
       "      <td>2.454545</td>\n",
       "      <td>2.136364</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>17.051905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>41</td>\n",
       "      <td>12.0</td>\n",
       "      <td>74</td>\n",
       "      <td>0.540541</td>\n",
       "      <td>3.729730</td>\n",
       "      <td>1.716216</td>\n",
       "      <td>1.337838</td>\n",
       "      <td>0.554054</td>\n",
       "      <td>11.774060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "17635      1                  26            16                       28.5   \n",
       "32078      1                  52            25                       18.0   \n",
       "1510       1                 111            67                       16.5   \n",
       "5272       1                  47            18                       23.5   \n",
       "26132      1                  99            41                       12.0   \n",
       "\n",
       "       num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "17635                12      0.750000    9.333333         4.000000   \n",
       "32078                35      0.485714    6.200000         2.571429   \n",
       "1510                 80      0.525000    3.987500         1.975000   \n",
       "5272                 22      1.000000    5.772727         2.454545   \n",
       "26132                74      0.540541    3.729730         1.716216   \n",
       "\n",
       "       verbs_third_person_rate  verbs_others_rate        ari  \n",
       "17635                 2.166667           1.333333  24.406113  \n",
       "32078                 1.485714           0.714286  20.586790  \n",
       "1510                  1.387500           0.837500  11.442837  \n",
       "5272                  2.136364           0.818182  17.051905  \n",
       "26132                 1.337838           0.554054  11.774060  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.pipeline = joblib.load(\"./data/knn_pipeline_ari.joblib\")\n",
    "print(predictor.data.shape)\n",
    "print(predictor.score())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running load_content_data first\n",
      "Complete process sentences\n",
      "finish load_content_string\n",
      "Complete process ari\n",
      "finish load_readability_string\n",
      "(1, 10)\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.610071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "0                  14             7                         27   \n",
       "\n",
       "   num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "0                 7      0.714286         9.0         2.857143   \n",
       "\n",
       "   verbs_third_person_rate  verbs_others_rate        ari  \n",
       "0                      2.0                1.0  18.610071  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.pipeline = joblib.load(\"./data/knn_pipeline_ari.joblib\")\n",
    "predictor.load_readability_string(test)\n",
    "print(predictor.data.shape)\n",
    "print(predictor.predict())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete process sentences\n",
      "finish load_content_string\n",
      "(1, 9)\n",
      "[0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "0                  14             7                         27   \n",
       "\n",
       "   num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "0                 7      0.714286         9.0         2.857143   \n",
       "\n",
       "   verbs_third_person_rate  verbs_others_rate  \n",
       "0                      2.0                1.0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.pipeline = joblib.load(\"./data/knn_pipeline.joblib\")\n",
    "predictor.load_content_string(test)\n",
    "print(predictor.data.shape)\n",
    "print(predictor.predict())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
