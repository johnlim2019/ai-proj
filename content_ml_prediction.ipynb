{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import statistics\n",
    "from readability import Readability\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pickle\n",
    "\n",
    "pipeline = joblib.load(\"./data/knn_pipeline.joblib\")\n",
    "\n",
    "# features to be used \n",
    "# verbs_third_person, verbs_others, words_per_sentence_median, num_of_sentences, adverbs_rate, nouns_rate, \n",
    "# adjectives_rate, verbs_third_person_rate, verbs_others_rate, automatic_readability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string into data point\n",
    "# run prediction\n",
    "class Content_Predictor:\n",
    "    def __init__(self, pipeline) -> None:\n",
    "        self.pipeline: Pipeline = pipeline\n",
    "        self.adverbs = [\"RB\", \"RBR\", \"RBS\", \"WRB\"]\n",
    "        self.nouns = [\"NN\", \"NNS\"]\n",
    "        self.proper_nouns = [\"NNP\", \"NNPS\"]\n",
    "        self.adjective = [\"JJ\", \"JJR\", \"JJS\"]\n",
    "        self.conjunctions = [\"CC\"]\n",
    "        self.verbs_third_person = [\"VB\", \"VBD\", \"VBG\", \"VBN\"]\n",
    "        self.verbs_others = [\"VBP\", \"VBZ\"]\n",
    "        self.data = pd.DataFrame()\n",
    "\n",
    "    def __combine_columns(self, row):\n",
    "        return str(row[\"title\"]) + \". \" + str(row[\"text\"])\n",
    "\n",
    "    def process_readability(self, text_iterator):\n",
    "        readability_ls = []\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove non ascii (language english only)\n",
    "                t = re.sub(r\"[^\\x00-\\x7F]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                if len(t) == 0:\n",
    "                    readability_ls.append(np.NaN)\n",
    "                else:\n",
    "                    r = Readability(t)\n",
    "                    readability_ls.append(r.ari().score)\n",
    "            except:\n",
    "                readability_ls.append(np.NaN)\n",
    "                # print(text)\n",
    "                continue\n",
    "        print(\"Complete process ari\")\n",
    "        return readability_ls\n",
    "\n",
    "    def process_content_sentences(self, text_iterator):\n",
    "        sentence_median_length = []\n",
    "        num_of_sentences = []\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove some supers\n",
    "                t = re.sub(\"[^a-zA-Z0-9\\.\\?\\!]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                # tokenise sentences\n",
    "                t = re.split(\"\\!|\\.|\\?\", t)\n",
    "                # strip sentence\n",
    "                t = [item.strip() for item in t]\n",
    "                # drop empty string\n",
    "                t = list(filter(lambda x: x != \"\", t))\n",
    "                # tokenise words\n",
    "                sentences_tokenised = []\n",
    "                sentences_lengths = []\n",
    "                for sentence in t:\n",
    "                    # tokenise\n",
    "                    s = sentence.split(\" \")\n",
    "                    sentences_lengths.append(len(s))\n",
    "                    sentences_tokenised.append(s)\n",
    "                # sentence features\n",
    "                sentence_median_length.append(statistics.median(sentences_lengths))\n",
    "                num_of_sentences.append(len(sentences_tokenised))\n",
    "                # print(sentences_tokenised)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                sentence_median_length.append(0)\n",
    "                num_of_sentences.append(0)\n",
    "                continue\n",
    "        print(\"Complete process sentences\")\n",
    "        return sentence_median_length, num_of_sentences\n",
    "\n",
    "    def __count_word_pos(self, doc, word_type):\n",
    "        count = 0\n",
    "        for word in doc:\n",
    "            if word[1] in word_type:\n",
    "                count += 1\n",
    "        return count\n",
    "\n",
    "    def process_pos(self, text_iterator):\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        text_processed = []  # no stop words, as pos tokens\n",
    "        for text in text_iterator:\n",
    "            try:\n",
    "                # remove punctuation\n",
    "                t = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "                # remove multiple spaces\n",
    "                t = re.sub(r\" +\", \" \", t)\n",
    "                # remove newline\n",
    "                t = re.sub(r\"\\n\", \" \", t)\n",
    "                # clear trailing whitespaces\n",
    "                t = t.strip()\n",
    "                # lowercase\n",
    "                t = t.lower()\n",
    "                # tokenise\n",
    "                t = t.split(\" \")\n",
    "                # drop empty string\n",
    "                t = list(filter(lambda x: x != \"\", t))\n",
    "                no_stop_words = []\n",
    "                for token in t:\n",
    "                    if token not in stop_words:\n",
    "                        no_stop_words.append(token)\n",
    "                try:\n",
    "                    text_processed.append(pos_tag(no_stop_words))\n",
    "                except:\n",
    "                    text_processed.append([])\n",
    "            except:\n",
    "                text_processed.append(None)\n",
    "                continue\n",
    "        return text_processed\n",
    "\n",
    "    def predict(self):\n",
    "        data_input = self.data.dropna()\n",
    "        prediction = self.pipeline.predict(data_input.drop(columns=\"label\").values)\n",
    "        return prediction\n",
    "\n",
    "    def score(self):\n",
    "        data_input = self.data.dropna()\n",
    "        score = self.pipeline.score(\n",
    "            data_input.drop(columns=\"label\").values, data_input[\"label\"].values\n",
    "        )\n",
    "        return score\n",
    "\n",
    "    def load_data(self, data: pd.DataFrame) -> None:\n",
    "        # we take in the dataframe of untouched csv\n",
    "        # df = pd.read_csv(\"./data/WELFake_Dataset.csv\")\n",
    "        self.data[\"label\"] = data[\"label\"]\n",
    "        dirty_merged_data = data.apply(self.__combine_columns, axis=1)\n",
    "        self.data[\"words_per_sentence_median\"], self.data[\"num_of_sentences\"] = (\n",
    "            self.process_content_sentences(dirty_merged_data)\n",
    "        )\n",
    "        pos_tokens = self.process_pos(dirty_merged_data)\n",
    "        self.data[\"adverbs\"] = [\n",
    "            self.__count_word_pos(doc, self.adverbs) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"nouns\"] = [\n",
    "            self.__count_word_pos(doc, self.nouns) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"adjectives\"] = [\n",
    "            self.__count_word_pos(doc, self.adjective) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"verbs_third_person\"] = [\n",
    "            self.__count_word_pos(doc, self.verbs_third_person) for doc in pos_tokens\n",
    "        ]\n",
    "        self.data[\"verbs_others\"] = [\n",
    "            self.__count_word_pos(doc, self.verbs_others) for doc in pos_tokens\n",
    "        ]\n",
    "\n",
    "        self.data[\"adverbs_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"adverbs\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"nouns_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"nouns\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"adjectives_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"adjectives\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"verbs_third_person_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"verbs_third_person\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        self.data[\"verbs_others_rate\"] = np.where(\n",
    "            self.data[\"num_of_sentences\"] == 0,\n",
    "            0,\n",
    "            self.data[\"verbs_others\"] / self.data[\"num_of_sentences\"],\n",
    "        )\n",
    "        print(self.data.shape)\n",
    "        self.data = self.data.drop(columns=[\"adverbs\", \"nouns\", \"adjectives\"])\n",
    "        print(self.data.shape)\n",
    "\n",
    "        # reorganise the table format.\n",
    "        self.data = self.data.reindex(\n",
    "            [\n",
    "                \"label\",\n",
    "                \"verbs_third_person\",\n",
    "                \"verbs_others\",\n",
    "                \"words_per_sentence_median\",\n",
    "                \"num_of_sentences\",\n",
    "                \"adverbs_rate\",\n",
    "                \"nouns_rate\",\n",
    "                \"adjectives_rate\",\n",
    "                \"verbs_third_person_rate\",\n",
    "                \"verbs_others_rate\",\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        return\n",
    "\n",
    "    def load_readability_data(self, data: pd.DataFrame):\n",
    "        dirty_merged_data = data.apply(self.__combine_columns, axis=1)\n",
    "        readability_ls = self.process_readability(dirty_merged_data)\n",
    "        self.data[\"ari\"] = readability_ls\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no median for empty data\n",
      "no median for empty data\n",
      "Complete process sentences\n",
      "(7213, 13)\n",
      "(7213, 10)\n"
     ]
    }
   ],
   "source": [
    "predictor = Content_Predictor(pipeline)\n",
    "data = pd.read_csv(\"./data/WELFake_Dataset.csv\")\n",
    "data = data.sample(frac=0.1)\n",
    "# test = [\"William Shakespeare was born on April 23, 1564, in Stratford-upon-Avon. The son of John Shakespeare and Mary Arden, he was probably educated at the King Edward VI Grammar School in Stratford, where he learned Latin and a little Greek and read the Roman dramatists. At eighteen, he married Anne Hathaway, a woman seven or eight years his senior. Together, they raised two daughters: Susanna, who was born in 1583, and Judith (whose twin brother died in boyhood), born in 1585. Little is known about Shakespeare’s activities between 1585 and 1592. Robert Greene’s A Groatsworth of Wit alludes to him as an actor and playwright. Shakespeare may have taught at school during this period, but it seems more probable that shortly after 1585 he went to London to begin his apprenticeship as an actor. Due to the plague, the London theaters were often closed between June 1592 and April 1594. During that period, Shakespeare probably had some income from his patron, Henry Wriothesley, earl of Southampton, to whom he dedicated his first two poems, Venus and Adonis (1593) and The Rape of Lucrece (1594). The former was a long narrative poem depicting the rejection of Venus by Adonis, his death, and the consequent disappearance of beauty from the world. Despite conservative objections to the poem’s glorification of sensuality, it was immensely popular and was reprinted six times during the nine years following its publication.\"]\n",
    "# test = [\"this is a test. um oops! not really? haha haha!\",\"thats the rub. whats a rub?? rubber# duck duck!\"]\n",
    "# predictor.process_content_sentences(test)\n",
    "# print(predictor.process_pos(test))\n",
    "# predictor.process_readability(test)\n",
    "predictor.load_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7213, 10)\n",
      "0.9606266463330099\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64586</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22782</th>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>5.548387</td>\n",
       "      <td>2.096774</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>0.580645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56589</th>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>19.0</td>\n",
       "      <td>68</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>5.911765</td>\n",
       "      <td>2.602941</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.088235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21010</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>15.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>4.815789</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>1.447368</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "64586      0                   7             7                       15.0   \n",
       "22782      0                  75            18                       18.0   \n",
       "56589      0                 119            74                       19.0   \n",
       "20171      1                   6             5                       14.0   \n",
       "21010      1                  55            19                       15.5   \n",
       "\n",
       "       num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "64586                 9      0.111111    3.666667         1.777778   \n",
       "22782                31      0.677419    5.548387         2.096774   \n",
       "56589                68      0.558824    5.911765         2.602941   \n",
       "20171                 7      0.285714    3.428571         2.142857   \n",
       "21010                38      0.657895    4.815789         1.473684   \n",
       "\n",
       "       verbs_third_person_rate  verbs_others_rate  \n",
       "64586                 0.777778           0.777778  \n",
       "22782                 2.419355           0.580645  \n",
       "56589                 1.750000           1.088235  \n",
       "20171                 0.857143           0.714286  \n",
       "21010                 1.447368           0.500000  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(predictor.data.shape)\n",
    "print(predictor.score())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete process ari\n"
     ]
    }
   ],
   "source": [
    "predictor.load_readability_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7213, 11)\n",
      "0.961556347853046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>verbs_third_person</th>\n",
       "      <th>verbs_others</th>\n",
       "      <th>words_per_sentence_median</th>\n",
       "      <th>num_of_sentences</th>\n",
       "      <th>adverbs_rate</th>\n",
       "      <th>nouns_rate</th>\n",
       "      <th>adjectives_rate</th>\n",
       "      <th>verbs_third_person_rate</th>\n",
       "      <th>verbs_others_rate</th>\n",
       "      <th>ari</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64586</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>1.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>8.595105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22782</th>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>18</td>\n",
       "      <td>18.0</td>\n",
       "      <td>31</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>5.548387</td>\n",
       "      <td>2.096774</td>\n",
       "      <td>2.419355</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>14.083138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56589</th>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>19.0</td>\n",
       "      <td>68</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>5.911765</td>\n",
       "      <td>2.602941</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.088235</td>\n",
       "      <td>14.084243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20171</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>2.142857</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>8.897773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21010</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>15.5</td>\n",
       "      <td>38</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>4.815789</td>\n",
       "      <td>1.473684</td>\n",
       "      <td>1.447368</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>17.007022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  verbs_third_person  verbs_others  words_per_sentence_median  \\\n",
       "64586      0                   7             7                       15.0   \n",
       "22782      0                  75            18                       18.0   \n",
       "56589      0                 119            74                       19.0   \n",
       "20171      1                   6             5                       14.0   \n",
       "21010      1                  55            19                       15.5   \n",
       "\n",
       "       num_of_sentences  adverbs_rate  nouns_rate  adjectives_rate  \\\n",
       "64586                 9      0.111111    3.666667         1.777778   \n",
       "22782                31      0.677419    5.548387         2.096774   \n",
       "56589                68      0.558824    5.911765         2.602941   \n",
       "20171                 7      0.285714    3.428571         2.142857   \n",
       "21010                38      0.657895    4.815789         1.473684   \n",
       "\n",
       "       verbs_third_person_rate  verbs_others_rate        ari  \n",
       "64586                 0.777778           0.777778   8.595105  \n",
       "22782                 2.419355           0.580645  14.083138  \n",
       "56589                 1.750000           1.088235  14.084243  \n",
       "20171                 0.857143           0.714286   8.897773  \n",
       "21010                 1.447368           0.500000  17.007022  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.pipeline = joblib.load(\"./data/knn_pipeline_ari.joblib\")\n",
    "print(predictor.data.shape)\n",
    "print(predictor.score())\n",
    "predictor.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
