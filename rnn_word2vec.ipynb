{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 23187010\n",
      "(72100, 3)\n"
     ]
    }
   ],
   "source": [
    "df_text = pd.read_pickle('./data/clean_title_text.pkl')\n",
    "model_text = Word2Vec( df_text[\"tokens\"],vector_size=100)\n",
    "print(\"text: {0}\".format(model_text.corpus_total_words))\n",
    "model_text.wv.save_word2vec_format(\"./data/model_text\")\n",
    "print(df_text.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_keyed_vectors = KeyedVectors.load_word2vec_format(\"./data/model_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "243.0\n",
      "12434\n",
      "812.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.5808e+04, 2.1656e+04, 1.4808e+04, 7.5390e+03, 4.5840e+03,\n",
       "        3.0570e+03, 1.7490e+03, 9.1400e+02, 4.6500e+02, 3.4300e+02,\n",
       "        1.9400e+02, 1.9600e+02, 1.1500e+02, 8.5000e+01, 8.5000e+01,\n",
       "        7.2000e+01, 4.5000e+01, 5.0000e+01, 4.5000e+01, 4.6000e+01,\n",
       "        2.9000e+01, 2.3000e+01, 1.6000e+01, 2.1000e+01, 2.7000e+01,\n",
       "        1.3000e+01, 1.5000e+01, 5.0000e+00, 7.0000e+00, 1.1000e+01,\n",
       "        7.0000e+00, 5.0000e+00, 1.0000e+00, 1.0000e+01, 3.0000e+00,\n",
       "        6.0000e+00, 7.0000e+00, 3.0000e+00, 4.0000e+00, 4.0000e+00,\n",
       "        3.0000e+00, 3.0000e+00, 1.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        3.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 3.0000e+00, 1.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([2.000000e+00, 1.263200e+02, 2.506400e+02, 3.749600e+02,\n",
       "        4.992800e+02, 6.236000e+02, 7.479200e+02, 8.722400e+02,\n",
       "        9.965600e+02, 1.120880e+03, 1.245200e+03, 1.369520e+03,\n",
       "        1.493840e+03, 1.618160e+03, 1.742480e+03, 1.866800e+03,\n",
       "        1.991120e+03, 2.115440e+03, 2.239760e+03, 2.364080e+03,\n",
       "        2.488400e+03, 2.612720e+03, 2.737040e+03, 2.861360e+03,\n",
       "        2.985680e+03, 3.110000e+03, 3.234320e+03, 3.358640e+03,\n",
       "        3.482960e+03, 3.607280e+03, 3.731600e+03, 3.855920e+03,\n",
       "        3.980240e+03, 4.104560e+03, 4.228880e+03, 4.353200e+03,\n",
       "        4.477520e+03, 4.601840e+03, 4.726160e+03, 4.850480e+03,\n",
       "        4.974800e+03, 5.099120e+03, 5.223440e+03, 5.347760e+03,\n",
       "        5.472080e+03, 5.596400e+03, 5.720720e+03, 5.845040e+03,\n",
       "        5.969360e+03, 6.093680e+03, 6.218000e+03, 6.342320e+03,\n",
       "        6.466640e+03, 6.590960e+03, 6.715280e+03, 6.839600e+03,\n",
       "        6.963920e+03, 7.088240e+03, 7.212560e+03, 7.336880e+03,\n",
       "        7.461200e+03, 7.585520e+03, 7.709840e+03, 7.834160e+03,\n",
       "        7.958480e+03, 8.082800e+03, 8.207120e+03, 8.331440e+03,\n",
       "        8.455760e+03, 8.580080e+03, 8.704400e+03, 8.828720e+03,\n",
       "        8.953040e+03, 9.077360e+03, 9.201680e+03, 9.326000e+03,\n",
       "        9.450320e+03, 9.574640e+03, 9.698960e+03, 9.823280e+03,\n",
       "        9.947600e+03, 1.007192e+04, 1.019624e+04, 1.032056e+04,\n",
       "        1.044488e+04, 1.056920e+04, 1.069352e+04, 1.081784e+04,\n",
       "        1.094216e+04, 1.106648e+04, 1.119080e+04, 1.131512e+04,\n",
       "        1.143944e+04, 1.156376e+04, 1.168808e+04, 1.181240e+04,\n",
       "        1.193672e+04, 1.206104e+04, 1.218536e+04, 1.230968e+04,\n",
       "        1.243400e+04]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = df_text['tokens'].apply(lambda x: len(x))\n",
    "\n",
    "# Calculate the median length\n",
    "median_length = lengths.median()\n",
    "max_length = lengths.max()\n",
    "min_length = lengths.min()\n",
    "\n",
    "truncate_length = int(lengths.quantile(0.95))\n",
    "\n",
    "print(min_length)\n",
    "print(median_length)\n",
    "print(max_length)\n",
    "print(lengths.quantile(0.95))\n",
    "\n",
    "\n",
    "plt.hist(lengths,bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72100\n",
      "torch.Size([72100, 813, 100])\n",
      "torch.Size([72100])\n"
     ]
    }
   ],
   "source": [
    "# test with smaller amount of data\n",
    "FRACTION = 1\n",
    "size = int(df_text.shape[0] * FRACTION)\n",
    "print(size)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "def text_to_tensor(text_list, merged_keyed_vectors,truncate_length):\n",
    "    vectors = [merged_keyed_vectors[word] for word in text_list[:truncate_length+1] if word in merged_keyed_vectors]\n",
    "    return torch.tensor(vectors)\n",
    "\n",
    "\n",
    "def stack_tensor(df: pd.DataFrame, col_name, word_model,truncate_length):\n",
    "    tensor_ls = []\n",
    "    label_ls = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        # print(row[df.columns.get_loc('class')])\n",
    "        # print(row[df.columns.get_loc(col_name)])\n",
    "        label_ls.append(row[df.columns.get_loc(\"class\")])\n",
    "        tensor_ls.append(text_to_tensor(row[df.columns.get_loc(col_name)], word_model, truncate_length=truncate_length))\n",
    "    padded_ls = pad_sequence(tensor_ls, batch_first=True)\n",
    "    return torch.tensor(label_ls), torch.stack(tuple(padded_ls))\n",
    "\n",
    "\n",
    "text_labels_tensor, text_tensor = stack_tensor(\n",
    "    df_text.iloc[:size, :], \n",
    "    col_name=\"tokens\", \n",
    "    word_model=merged_keyed_vectors,\n",
    "    truncate_length=truncate_length\n",
    ")\n",
    "print(text_tensor.size())\n",
    "print(text_labels_tensor.size())\n",
    "text_dataset = TensorDataset(text_tensor, text_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_size 72100\n",
      "train_size 57680\n",
      "test_size 14420\n"
     ]
    }
   ],
   "source": [
    "def split(tensor_dataset, test_split=0.2):\n",
    "    test_size = int(len(tensor_dataset) * test_split)\n",
    "    train_size = int(len(tensor_dataset) - test_size)\n",
    "    train_data, test_data = random_split(tensor_dataset, [train_size, test_size])\n",
    "    print(f'total_size {len(tensor_dataset)}')\n",
    "    print(f'train_size {len(train_data)}')\n",
    "    print(f'test_size {len(test_data)}')\n",
    "    return train_data, test_data\n",
    "\n",
    "train_text_tensor, test_text_tensor = split(text_dataset,0.2)\n",
    "\n",
    "# with open(\"./data/train_merged\",\"wb\") as f:\n",
    "#     pickle.dump(train_text_tensor,f)\n",
    "# with open(\"./data/test_merged\",\"wb\") as f:\n",
    "#     pickle.dump(test_text_tensor,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([32, 813, 100])\n",
      "32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size=32\n",
    "train_text_data = DataLoader(train_text_tensor, batch_size=batch_size, shuffle=True)\n",
    "test_text_data = DataLoader(test_text_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# check batch dimension\n",
    "batch_size = train_text_data.batch_size\n",
    "for data, label in train_text_data:\n",
    "    print(\"shape: {0}\".format(data.size()))\n",
    "    break\n",
    "sequence_length = data.size()[1]\n",
    "print(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"mps\")\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self, hidden_size, input_size, num_layers, num_classes, dropout, activation_fn\n",
    "    ):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size * sequence_length, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.nonlinearity = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with random values.\n",
    "        h0 = torch.randn(self.gru.num_layers, x.size(0), self.gru.hidden_size).to(gpu)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.nonlinearity(out)\n",
    "        out_distribution = nn.functional.log_softmax(out, dim=-1)\n",
    "        return out_distribution\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader, test_dataloader, loss_criterion, num_epochs, model, optimizer\n",
    "):\n",
    "    # A counter for the number of gradient updates we've performed.\n",
    "    num_iter = 0\n",
    "\n",
    "    # Iterate `num_epochs` times.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting epoch {}\".format(epoch + 1))\n",
    "        # Iterate over the train_dataloader, unpacking the images and labels\n",
    "        for data, labels in train_dataloader:\n",
    "            # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
    "            if gpu:\n",
    "                data = data.to(gpu)\n",
    "                labels = labels.to(gpu)\n",
    "\n",
    "            # Run the forward pass through the model to get predicted log distribution.\n",
    "            predicted = model(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            batch_loss = loss_criterion(predicted, labels)\n",
    "\n",
    "            # Clear the gradients as we prepare to backprop.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backprop (backward pass), which calculates gradients.\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Take a gradient step to update parameters.\n",
    "            optimizer.step()\n",
    "\n",
    "            # Increment gradient update counter.\n",
    "            num_iter += 1\n",
    "\n",
    "            # Calculate test set loss and accuracy every 100 gradient updates\n",
    "            # It's standard to have this as a separate evaluate function, but\n",
    "            # we'll place it inline for didactic purposes.\n",
    "            if num_iter % 1000 == 0:\n",
    "                # Set model to eval mode, which turns off dropout.\n",
    "                model.eval()\n",
    "                # Counters for the num of examples we get right / total num of examples.\n",
    "                num_correct = 0\n",
    "                total_examples = 0\n",
    "                total_test_loss = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Iterate over the test dataloader\n",
    "                    for test_data, test_labels in test_dataloader:\n",
    "\n",
    "                        # If we're using the GPU, move tensors to the GPU.\n",
    "                        if gpu:\n",
    "                            test_data = test_data.to(gpu)\n",
    "                            test_labels = test_labels.to(gpu)\n",
    "\n",
    "                        # Run the forward pass to get predicted distribution.\n",
    "                        predicted = model(test_data)\n",
    "\n",
    "                        # Calculate loss for this test batch. This is averaged, so multiply\n",
    "                        # by the number of examples in batch to get a total.\n",
    "                        total_test_loss += loss_criterion(\n",
    "                            predicted, test_labels\n",
    "                        ).data * test_labels.size(0)\n",
    "\n",
    "                        # Get predicted labels (argmax)\n",
    "                        _, predicted_labels = torch.max(predicted.data, 1)\n",
    "\n",
    "                        # Count the number of examples in this batch\n",
    "                        total_examples += test_labels.size(0)\n",
    "\n",
    "                        # Count the total number of correctly predicted labels.\n",
    "                        # predicted == labels generates a ByteTensor in indices where\n",
    "                        # predicted and labels match, so we can sum to get the num correct.\n",
    "                        num_correct += torch.sum(predicted_labels == test_labels.data)\n",
    "                accuracy = 100 * num_correct / total_examples\n",
    "                average_test_loss = total_test_loss / total_examples\n",
    "                print(\n",
    "                    \"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
    "                        num_iter, average_test_loss, accuracy\n",
    "                    )\n",
    "                )\n",
    "                # Set the model back to train mode, which activates dropout again.\n",
    "                model.train()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (gru): GRU(100, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=104064, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (nonlinearity): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_fn = nn.ReLU()\n",
    "model = RNNClassifier(\n",
    "    hidden_size=128, input_size=100, num_layers=1, num_classes=2, dropout=0.5, activation_fn=activation_fn\n",
    ")\n",
    "loss_criterion = nn.NLLLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size:32\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1000. Test Loss 0.45920586585998535. Test Accuracy 92.45492553710938.\n",
      "Starting epoch 2\n",
      "Iteration 2000. Test Loss 0.4278078079223633. Test Accuracy 94.02912902832031.\n",
      "Iteration 3000. Test Loss 0.4349280595779419. Test Accuracy 94.09847259521484.\n",
      "Starting epoch 3\n",
      "Iteration 4000. Test Loss 0.41848501563072205. Test Accuracy 94.9930648803711.\n",
      "Iteration 5000. Test Loss 0.4141189157962799. Test Accuracy 94.81969451904297.\n",
      "Starting epoch 4\n",
      "Iteration 6000. Test Loss 0.4041016101837158. Test Accuracy 95.53398132324219.\n",
      "Iteration 7000. Test Loss 0.1282380223274231. Test Accuracy 95.59639739990234.\n",
      "Starting epoch 5\n",
      "Iteration 8000. Test Loss 0.11466962099075317. Test Accuracy 95.77669525146484.\n",
      "Iteration 9000. Test Loss 0.11623404175043106. Test Accuracy 95.8460464477539.\n",
      "Starting epoch 6\n",
      "Iteration 10000. Test Loss 0.10595009475946426. Test Accuracy 95.99861145019531.\n",
      "Starting epoch 7\n",
      "Iteration 11000. Test Loss 0.10443523526191711. Test Accuracy 95.92926788330078.\n",
      "Iteration 12000. Test Loss 0.10569700598716736. Test Accuracy 96.02635192871094.\n",
      "Starting epoch 8\n",
      "Iteration 13000. Test Loss 0.10071760416030884. Test Accuracy 95.92926788330078.\n",
      "Iteration 14000. Test Loss 0.09553711861371994. Test Accuracy 96.44937896728516.\n",
      "Starting epoch 9\n",
      "Iteration 15000. Test Loss 0.0916677936911583. Test Accuracy 96.60887908935547.\n",
      "Iteration 16000. Test Loss 0.0868038460612297. Test Accuracy 96.8238525390625.\n",
      "Starting epoch 10\n",
      "Iteration 17000. Test Loss 0.09141634404659271. Test Accuracy 96.58807373046875.\n",
      "Iteration 18000. Test Loss 0.08691833168268204. Test Accuracy 96.76837921142578.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "print(f\"batch size:{batch_size}\")\n",
    "model = train(\n",
    "    train_text_data, \n",
    "    test_text_data, \n",
    "    loss_criterion, \n",
    "    num_epochs, \n",
    "    model, \n",
    "    optimiser\n",
    ")\n",
    "with open(\"./data/model\",\"wb\") as f:\n",
    "    torch.save(model,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
