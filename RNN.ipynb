{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:\n",
      "2.2.0\n",
      "GPU Detected:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(\"PyTorch version:\")\n",
    "print(torch.__version__)\n",
    "print(\"GPU Detected:\")\n",
    "# print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# defining a shortcut function for later:\n",
    "import os\n",
    "\n",
    "# gpu = torch.device(\"cuda:0\")\n",
    "gpu = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_text\", \"rb\") as f:\n",
    "    train_text = pickle.load(f)\n",
    "with open(\"./data/test_text\", \"rb\") as f:\n",
    "    test_text = pickle.load(f)\n",
    "\n",
    "train_text_data = DataLoader(train_text, batch_size=1, shuffle=True)\n",
    "test_text_data = DataLoader(test_text, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571\n",
      "142\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_data.dataset))\n",
    "print(len(test_text_data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 12203, 300])\n"
     ]
    }
   ],
   "source": [
    "# check batch dimension\n",
    "batch_size = train_text_data.batch_size\n",
    "for data, label in train_text_data:\n",
    "    print(\"shape: {0}\".format(data.size()))\n",
    "    break\n",
    "sequence_length = data.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, sequence_size, kernel_size, num_class, dropout, activation_fn):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(sequence_size, 512, kernel_size=kernel_size)\n",
    "        )  # layer1\n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(512, 256, kernel_size=kernel_size)\n",
    "        )  # layer2\n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(256, 128, kernel_size=kernel_size)\n",
    "        )  # layer3\n",
    "        self.flatten = nn.Flatten()  # Flatten layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # calculate the size of the flatten operation. \n",
    "        # vw size + 2 * padding - dilation(kernel -1 ) -1 )/ stride + 1\n",
    "        L_out = floor((300 +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        L_out = floor((L_out +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        L_out = floor((L_out +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        print(L_out)\n",
    "        self.output_projection = nn.Linear(128 * L_out, num_class)\n",
    "        self.nonlinearity = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.nonlinearity(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        out = self.output_projection(x)\n",
    "\n",
    "        out_distribution = nn.functional.log_softmax(out, dim=-1)\n",
    "        return out_distribution\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader, test_dataloader, nll_criterion, num_epochs, ffnn, ffnn_optimizer\n",
    "):\n",
    "    # A counter for the number of gradient updates we've performed.\n",
    "    num_iter = 0\n",
    "\n",
    "    # Iterate `num_epochs` times.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting epoch {}\".format(epoch + 1))\n",
    "        # Iterate over the train_dataloader, unpacking the images and labels\n",
    "        for data, labels in train_dataloader:\n",
    "            # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
    "            if gpu:\n",
    "                data = data.to(gpu)\n",
    "                labels = labels.to(gpu)\n",
    "\n",
    "            # Run the forward pass through the model to get predicted log distribution.\n",
    "            predicted = ffnn(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            batch_loss = nll_criterion(predicted, labels)\n",
    "\n",
    "            # Clear the gradients as we prepare to backprop.\n",
    "            ffnn_optimizer.zero_grad()\n",
    "\n",
    "            # Backprop (backward pass), which calculates gradients.\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Take a gradient step to update parameters.\n",
    "            ffnn_optimizer.step()\n",
    "\n",
    "            # Increment gradient update counter.\n",
    "            num_iter += 1\n",
    "\n",
    "            # Calculate test set loss and accuracy every 500 gradient updates\n",
    "            # It's standard to have this as a separate evaluate function, but\n",
    "            # we'll place it inline for didactic purposes.\n",
    "            if num_iter % 500 == 0:\n",
    "                # Set model to eval mode, which turns off dropout.\n",
    "                ffnn.eval()\n",
    "                # Counters for the num of examples we get right / total num of examples.\n",
    "                num_correct = 0\n",
    "                total_examples = 0\n",
    "                total_test_loss = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Iterate over the test dataloader\n",
    "                    for test_data, test_labels in test_dataloader:\n",
    "\n",
    "                        # If we're using the GPU, move tensors to the GPU.\n",
    "                        if gpu:\n",
    "                            test_data = test_data.to(gpu)\n",
    "                            test_labels = test_labels.to(gpu)\n",
    "\n",
    "                        # Run the forward pass to get predicted distribution.\n",
    "                        predicted = ffnn(test_data)\n",
    "\n",
    "                        # Calculate loss for this test batch. This is averaged, so multiply\n",
    "                        # by the number of examples in batch to get a total.\n",
    "                        total_test_loss += nll_criterion(\n",
    "                            predicted, test_labels\n",
    "                        ).data * test_labels.size(0)\n",
    "\n",
    "                        # Get predicted labels (argmax)\n",
    "                        _, predicted_labels = torch.max(predicted.data, 1)\n",
    "\n",
    "                        # Count the number of examples in this batch\n",
    "                        total_examples += test_labels.size(0)\n",
    "\n",
    "                        # Count the total number of correctly predicted labels.\n",
    "                        # predicted == labels generates a ByteTensor in indices where\n",
    "                        # predicted and labels match, so we can sum to get the num correct.\n",
    "                        num_correct += torch.sum(predicted_labels == test_labels.data)\n",
    "                accuracy = 100 * num_correct / total_examples\n",
    "                average_test_loss = total_test_loss / total_examples\n",
    "                print(\n",
    "                    \"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
    "                        num_iter, average_test_loss, accuracy\n",
    "                    )\n",
    "                )\n",
    "                # Set the model back to train mode, which activates dropout again.\n",
    "                ffnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/limjohn/.pyenv/versions/3.10.12/envs/ai-course/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Conv1d(12203, 512, kernel_size=(10,), stride=(1,))\n",
       "    (1): Conv1d(512, 256, kernel_size=(10,), stride=(1,))\n",
       "    (2): Conv1d(256, 128, kernel_size=(10,), stride=(1,))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (output_projection): Linear(in_features=34944, out_features=2, bias=True)\n",
       "  (nonlinearity): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNNClassifier(\n",
    "    sequence_size=sequence_length, num_class=2, kernel_size=10, dropout=0.5, activation_fn=nn.ReLU()\n",
    ")\n",
    "nll_criterion = nn.NLLLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Iteration 500. Test Loss 0.6690066456794739. Test Accuracy 61.971832275390625.\n",
      "Starting epoch 2\n",
      "Iteration 1000. Test Loss 0.592875599861145. Test Accuracy 71.12676239013672.\n",
      "Starting epoch 3\n",
      "Iteration 1500. Test Loss 0.5499732494354248. Test Accuracy 79.57746124267578.\n",
      "Starting epoch 4\n",
      "Iteration 2000. Test Loss 0.5663434863090515. Test Accuracy 68.30986022949219.\n",
      "Starting epoch 5\n",
      "Iteration 2500. Test Loss 0.48230302333831787. Test Accuracy 75.35211181640625.\n",
      "Starting epoch 6\n",
      "Iteration 3000. Test Loss 0.44471484422683716. Test Accuracy 81.69013977050781.\n",
      "Starting epoch 7\n",
      "Iteration 3500. Test Loss 0.434649795293808. Test Accuracy 84.50704193115234.\n",
      "Starting epoch 8\n",
      "Iteration 4000. Test Loss 0.5968635082244873. Test Accuracy 74.64788818359375.\n",
      "Iteration 4500. Test Loss 0.569172203540802. Test Accuracy 66.90140533447266.\n",
      "Starting epoch 9\n",
      "Iteration 5000. Test Loss 0.702302873134613. Test Accuracy 83.09859466552734.\n",
      "Starting epoch 10\n",
      "Iteration 5500. Test Loss 0.534611701965332. Test Accuracy 75.35211181640625.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(\n",
    "    train_text_data, \n",
    "    test_text_data, \n",
    "    nll_criterion, \n",
    "    num_epochs, \n",
    "    model, \n",
    "    optimiser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
