{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:\n",
      "2.2.0\n",
      "GPU Detected:\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "print(\"PyTorch version:\")\n",
    "print(torch.__version__)\n",
    "print(\"GPU Detected:\")\n",
    "# print(torch.cuda.is_available())\n",
    "print(torch.backends.mps.is_available())\n",
    "\n",
    "# defining a shortcut function for later:\n",
    "import os\n",
    "\n",
    "# gpu = torch.device(\"cuda:0\")\n",
    "gpu = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/train_text\", \"rb\") as f:\n",
    "    train_text = pickle.load(f)\n",
    "with open(\"./data/test_text\", \"rb\") as f:\n",
    "    test_text = pickle.load(f)\n",
    "\n",
    "train_text_data = DataLoader(train_text, batch_size=1, shuffle=True)\n",
    "test_text_data = DataLoader(test_text, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5707\n",
      "1426\n"
     ]
    }
   ],
   "source": [
    "print(len(train_text_data.dataset))\n",
    "print(len(test_text_data.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 807, 300])\n"
     ]
    }
   ],
   "source": [
    "# check batch dimension\n",
    "batch_size = train_text_data.batch_size\n",
    "for data, label in train_text_data:\n",
    "    print(\"shape: {0}\".format(data.size()))\n",
    "    break\n",
    "sequence_length = data.size()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "    def __init__(self, sequence_size, kernel_size, num_class, dropout, activation_fn):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(sequence_size, 512, kernel_size=kernel_size)\n",
    "        )  # layer1        \n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(512, 256, kernel_size=kernel_size)\n",
    "        )  # layer2\n",
    "        self.hidden_layers.append(\n",
    "            nn.Conv1d(256, 128, kernel_size=kernel_size)\n",
    "        )  # layer3\n",
    "        self.flatten = nn.Flatten()  # Flatten layer\n",
    "        # calculate the size of the flatten operation. \n",
    "        # vw size + 2 * padding - dilation(kernel -1 ) -1 )/ stride + 1\n",
    "        L_out = floor((300 +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        L_out = floor((L_out +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        L_out = floor((L_out +   2*0 -   1*(kernel_size -   1) - 1) /   1 +   1)\n",
    "        print(L_out)\n",
    "        self.output_projection = nn.Linear(128 * L_out, num_class)\n",
    "        self.nonlinearity = activation_fn\n",
    "        self.dropout = nn.Dropout1d(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for index,hidden_layer in enumerate(self.hidden_layers):\n",
    "            x = hidden_layer(x)\n",
    "            x = self.nonlinearity(x)\n",
    "            x = self.dropout(x)\n",
    "                        \n",
    "\n",
    "        x = self.flatten(x)\n",
    "        out = self.output_projection(x)\n",
    "\n",
    "        out_distribution = nn.functional.log_softmax(out, dim=-1)\n",
    "        return out_distribution\n",
    "\n",
    "\n",
    "def train(\n",
    "    train_dataloader, test_dataloader, nll_criterion, num_epochs, ffnn, ffnn_optimizer\n",
    "):\n",
    "    # A counter for the number of gradient updates we've performed.\n",
    "    num_iter = 0\n",
    "\n",
    "    # Iterate `num_epochs` times.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Starting epoch {}\".format(epoch + 1))\n",
    "        # Iterate over the train_dataloader, unpacking the images and labels\n",
    "        for data, labels in train_dataloader:\n",
    "            # If we're using the GPU, move reshaped_images and labels to the GPU.\n",
    "            if gpu:\n",
    "                data = data.to(gpu)\n",
    "                labels = labels.to(gpu)\n",
    "\n",
    "            # Run the forward pass through the model to get predicted log distribution.\n",
    "            predicted = ffnn(data)\n",
    "\n",
    "            # Calculate the loss\n",
    "            batch_loss = nll_criterion(predicted, labels)\n",
    "\n",
    "            # Clear the gradients as we prepare to backprop.\n",
    "            ffnn_optimizer.zero_grad()\n",
    "\n",
    "            # Backprop (backward pass), which calculates gradients.\n",
    "            batch_loss.backward()\n",
    "\n",
    "            # Take a gradient step to update parameters.\n",
    "            ffnn_optimizer.step()\n",
    "\n",
    "            # Increment gradient update counter.\n",
    "            num_iter += 1\n",
    "\n",
    "            # Calculate test set loss and accuracy every 500 gradient updates\n",
    "            # It's standard to have this as a separate evaluate function, but\n",
    "            # we'll place it inline for didactic purposes.\n",
    "            if num_iter % 500 == 0:\n",
    "                # Set model to eval mode, which turns off dropout.\n",
    "                ffnn.eval()\n",
    "                # Counters for the num of examples we get right / total num of examples.\n",
    "                num_correct = 0\n",
    "                total_examples = 0\n",
    "                total_test_loss = 0\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # Iterate over the test dataloader\n",
    "                    for test_data, test_labels in test_dataloader:\n",
    "\n",
    "                        # If we're using the GPU, move tensors to the GPU.\n",
    "                        if gpu:\n",
    "                            test_data = test_data.to(gpu)\n",
    "                            test_labels = test_labels.to(gpu)\n",
    "\n",
    "                        # Run the forward pass to get predicted distribution.\n",
    "                        predicted = ffnn(test_data)\n",
    "\n",
    "                        # Calculate loss for this test batch. This is averaged, so multiply\n",
    "                        # by the number of examples in batch to get a total.\n",
    "                        total_test_loss += nll_criterion(\n",
    "                            predicted, test_labels\n",
    "                        ).data * test_labels.size(0)\n",
    "\n",
    "                        # Get predicted labels (argmax)\n",
    "                        _, predicted_labels = torch.max(predicted.data, 1)\n",
    "\n",
    "                        # Count the number of examples in this batch\n",
    "                        total_examples += test_labels.size(0)\n",
    "\n",
    "                        # Count the total number of correctly predicted labels.\n",
    "                        # predicted == labels generates a ByteTensor in indices where\n",
    "                        # predicted and labels match, so we can sum to get the num correct.\n",
    "                        num_correct += torch.sum(predicted_labels == test_labels.data)\n",
    "                accuracy = 100 * num_correct / total_examples\n",
    "                average_test_loss = total_test_loss / total_examples\n",
    "                print(\n",
    "                    \"Iteration {}. Test Loss {}. Test Accuracy {}.\".format(\n",
    "                        num_iter, average_test_loss, accuracy\n",
    "                    )\n",
    "                )\n",
    "                # Set the model back to train mode, which activates dropout again.\n",
    "                ffnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TextClassificationModel(\n",
       "  (hidden_layers): ModuleList(\n",
       "    (0): Conv1d(807, 512, kernel_size=(10,), stride=(1,))\n",
       "    (1): Conv1d(512, 256, kernel_size=(10,), stride=(1,))\n",
       "    (2): Conv1d(256, 128, kernel_size=(10,), stride=(1,))\n",
       "  )\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (output_projection): Linear(in_features=31488, out_features=2, bias=True)\n",
       "  (nonlinearity): ReLU()\n",
       "  (pooling_layers): ModuleList(\n",
       "    (0-2): 3 x MaxPool1d(kernel_size=10, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextClassificationModel(\n",
    "    sequence_size=sequence_length, num_class=2, kernel_size=10, activation_fn=nn.ReLU()\n",
    ")\n",
    "nll_criterion = nn.NLLLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "model.to(gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Iteration 500. Test Loss 0.6921217441558838. Test Accuracy 53.43618392944336.\n",
      "Iteration 1000. Test Loss 0.691042423248291. Test Accuracy 53.43618392944336.\n",
      "Iteration 1500. Test Loss 0.690847635269165. Test Accuracy 53.43618392944336.\n",
      "Iteration 2000. Test Loss 0.6920719146728516. Test Accuracy 53.43618392944336.\n",
      "Iteration 2500. Test Loss 0.6957212090492249. Test Accuracy 46.56381607055664.\n",
      "Iteration 3000. Test Loss 0.6919916272163391. Test Accuracy 53.43618392944336.\n",
      "Iteration 3500. Test Loss 0.692599356174469. Test Accuracy 53.43618392944336.\n",
      "Iteration 4000. Test Loss 0.692149817943573. Test Accuracy 53.43618392944336.\n",
      "Iteration 4500. Test Loss 0.6914863586425781. Test Accuracy 53.43618392944336.\n",
      "Iteration 5000. Test Loss 0.6917837262153625. Test Accuracy 53.43618392944336.\n",
      "Iteration 5500. Test Loss 0.6911836266517639. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 2\n",
      "Iteration 6000. Test Loss 0.6916190385818481. Test Accuracy 53.43618392944336.\n",
      "Iteration 6500. Test Loss 0.6917867660522461. Test Accuracy 53.43618392944336.\n",
      "Iteration 7000. Test Loss 0.6914875507354736. Test Accuracy 53.43618392944336.\n",
      "Iteration 7500. Test Loss 0.6920408010482788. Test Accuracy 53.43618392944336.\n",
      "Iteration 8000. Test Loss 0.6920217275619507. Test Accuracy 53.43618392944336.\n",
      "Iteration 8500. Test Loss 0.6912124156951904. Test Accuracy 53.43618392944336.\n",
      "Iteration 9000. Test Loss 0.6910231709480286. Test Accuracy 53.43618392944336.\n",
      "Iteration 9500. Test Loss 0.6912209987640381. Test Accuracy 53.43618392944336.\n",
      "Iteration 10000. Test Loss 0.6910701990127563. Test Accuracy 53.43618392944336.\n",
      "Iteration 10500. Test Loss 0.6909112334251404. Test Accuracy 53.43618392944336.\n",
      "Iteration 11000. Test Loss 0.6908978223800659. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 3\n",
      "Iteration 11500. Test Loss 0.6911184787750244. Test Accuracy 53.43618392944336.\n",
      "Iteration 12000. Test Loss 0.6909324526786804. Test Accuracy 53.43618392944336.\n",
      "Iteration 12500. Test Loss 0.6908459067344666. Test Accuracy 53.43618392944336.\n",
      "Iteration 13000. Test Loss 0.6912245750427246. Test Accuracy 53.43618392944336.\n",
      "Iteration 13500. Test Loss 0.6918053030967712. Test Accuracy 53.43618392944336.\n",
      "Iteration 14000. Test Loss 0.6916164755821228. Test Accuracy 53.43618392944336.\n",
      "Iteration 14500. Test Loss 0.6911429762840271. Test Accuracy 53.43618392944336.\n",
      "Iteration 15000. Test Loss 0.6911723613739014. Test Accuracy 53.43618392944336.\n",
      "Iteration 15500. Test Loss 0.691038966178894. Test Accuracy 53.43618392944336.\n",
      "Iteration 16000. Test Loss 0.6908406615257263. Test Accuracy 53.43618392944336.\n",
      "Iteration 16500. Test Loss 0.6910536289215088. Test Accuracy 53.43618392944336.\n",
      "Iteration 17000. Test Loss 0.6912045478820801. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 4\n",
      "Iteration 17500. Test Loss 0.69118332862854. Test Accuracy 53.43618392944336.\n",
      "Iteration 18000. Test Loss 0.6908799409866333. Test Accuracy 53.43618392944336.\n",
      "Iteration 18500. Test Loss 0.6909834742546082. Test Accuracy 53.43618392944336.\n",
      "Iteration 19000. Test Loss 0.6910553574562073. Test Accuracy 53.43618392944336.\n",
      "Iteration 19500. Test Loss 0.6914265155792236. Test Accuracy 53.43618392944336.\n",
      "Iteration 20000. Test Loss 0.6916120052337646. Test Accuracy 53.43618392944336.\n",
      "Iteration 20500. Test Loss 0.6909481287002563. Test Accuracy 53.43618392944336.\n",
      "Iteration 21000. Test Loss 0.6908625960350037. Test Accuracy 53.43618392944336.\n",
      "Iteration 21500. Test Loss 0.6910896897315979. Test Accuracy 53.43618392944336.\n",
      "Iteration 22000. Test Loss 0.6911801099777222. Test Accuracy 53.43618392944336.\n",
      "Iteration 22500. Test Loss 0.6913439035415649. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 5\n",
      "Iteration 23000. Test Loss 0.6918136477470398. Test Accuracy 53.43618392944336.\n",
      "Iteration 23500. Test Loss 0.691569983959198. Test Accuracy 53.43618392944336.\n",
      "Iteration 24000. Test Loss 0.6918743848800659. Test Accuracy 53.43618392944336.\n",
      "Iteration 24500. Test Loss 0.6910648345947266. Test Accuracy 53.43618392944336.\n",
      "Iteration 25000. Test Loss 0.6912639737129211. Test Accuracy 53.43618392944336.\n",
      "Iteration 25500. Test Loss 0.6910639405250549. Test Accuracy 53.43618392944336.\n",
      "Iteration 26000. Test Loss 0.6916736364364624. Test Accuracy 53.43618392944336.\n",
      "Iteration 26500. Test Loss 0.691460371017456. Test Accuracy 53.43618392944336.\n",
      "Iteration 27000. Test Loss 0.691390872001648. Test Accuracy 53.43618392944336.\n",
      "Iteration 27500. Test Loss 0.691512405872345. Test Accuracy 53.43618392944336.\n",
      "Iteration 28000. Test Loss 0.6911599636077881. Test Accuracy 53.43618392944336.\n",
      "Iteration 28500. Test Loss 0.6911429762840271. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 6\n",
      "Iteration 29000. Test Loss 0.6913704872131348. Test Accuracy 53.43618392944336.\n",
      "Iteration 29500. Test Loss 0.6914539933204651. Test Accuracy 53.43618392944336.\n",
      "Iteration 30000. Test Loss 0.6922914981842041. Test Accuracy 53.43618392944336.\n",
      "Iteration 30500. Test Loss 0.691886305809021. Test Accuracy 53.43618392944336.\n",
      "Iteration 31000. Test Loss 0.6915681958198547. Test Accuracy 53.43618392944336.\n",
      "Iteration 31500. Test Loss 0.6914394497871399. Test Accuracy 53.43618392944336.\n",
      "Iteration 32000. Test Loss 0.6914021968841553. Test Accuracy 53.43618392944336.\n",
      "Iteration 32500. Test Loss 0.6915285587310791. Test Accuracy 53.43618392944336.\n",
      "Iteration 33000. Test Loss 0.6910986304283142. Test Accuracy 53.43618392944336.\n",
      "Iteration 33500. Test Loss 0.6907894611358643. Test Accuracy 53.43618392944336.\n",
      "Iteration 34000. Test Loss 0.6908983588218689. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 7\n",
      "Iteration 34500. Test Loss 0.6908079981803894. Test Accuracy 53.43618392944336.\n",
      "Iteration 35000. Test Loss 0.6907916069030762. Test Accuracy 53.43618392944336.\n",
      "Iteration 35500. Test Loss 0.6912262439727783. Test Accuracy 53.43618392944336.\n",
      "Iteration 36000. Test Loss 0.6911177635192871. Test Accuracy 53.43618392944336.\n",
      "Iteration 36500. Test Loss 0.6912104487419128. Test Accuracy 53.43618392944336.\n",
      "Iteration 37000. Test Loss 0.6914868950843811. Test Accuracy 53.43618392944336.\n",
      "Iteration 37500. Test Loss 0.6912027597427368. Test Accuracy 53.43618392944336.\n",
      "Iteration 38000. Test Loss 0.6911187767982483. Test Accuracy 53.43618392944336.\n",
      "Iteration 38500. Test Loss 0.6911965012550354. Test Accuracy 53.43618392944336.\n",
      "Iteration 39000. Test Loss 0.6912019848823547. Test Accuracy 53.43618392944336.\n",
      "Iteration 39500. Test Loss 0.6910005807876587. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 8\n",
      "Iteration 40000. Test Loss 0.6912999153137207. Test Accuracy 53.43618392944336.\n",
      "Iteration 40500. Test Loss 0.6915290355682373. Test Accuracy 53.43618392944336.\n",
      "Iteration 41000. Test Loss 0.6908514499664307. Test Accuracy 53.43618392944336.\n",
      "Iteration 41500. Test Loss 0.6908243894577026. Test Accuracy 53.43618392944336.\n",
      "Iteration 42000. Test Loss 0.6909620761871338. Test Accuracy 53.43618392944336.\n",
      "Iteration 42500. Test Loss 0.6912508606910706. Test Accuracy 53.43618392944336.\n",
      "Iteration 43000. Test Loss 0.6909981369972229. Test Accuracy 53.43618392944336.\n",
      "Iteration 43500. Test Loss 0.6911976337432861. Test Accuracy 53.43618392944336.\n",
      "Iteration 44000. Test Loss 0.6911044120788574. Test Accuracy 53.43618392944336.\n",
      "Iteration 44500. Test Loss 0.6912165880203247. Test Accuracy 53.43618392944336.\n",
      "Iteration 45000. Test Loss 0.6917235255241394. Test Accuracy 53.43618392944336.\n",
      "Iteration 45500. Test Loss 0.6912966966629028. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 9\n",
      "Iteration 46000. Test Loss 0.691146731376648. Test Accuracy 53.43618392944336.\n",
      "Iteration 46500. Test Loss 0.6916434168815613. Test Accuracy 53.43618392944336.\n",
      "Iteration 47000. Test Loss 0.6913679242134094. Test Accuracy 53.43618392944336.\n",
      "Iteration 47500. Test Loss 0.6914281845092773. Test Accuracy 53.43618392944336.\n",
      "Iteration 48000. Test Loss 0.6909952163696289. Test Accuracy 53.43618392944336.\n",
      "Iteration 48500. Test Loss 0.6909193992614746. Test Accuracy 53.43618392944336.\n",
      "Iteration 49000. Test Loss 0.6910651922225952. Test Accuracy 53.43618392944336.\n",
      "Iteration 49500. Test Loss 0.691641628742218. Test Accuracy 53.43618392944336.\n",
      "Iteration 50000. Test Loss 0.6919838786125183. Test Accuracy 53.43618392944336.\n",
      "Iteration 50500. Test Loss 0.6920568943023682. Test Accuracy 53.43618392944336.\n",
      "Iteration 51000. Test Loss 0.6914663910865784. Test Accuracy 53.43618392944336.\n",
      "Starting epoch 10\n",
      "Iteration 51500. Test Loss 0.691129207611084. Test Accuracy 53.43618392944336.\n",
      "Iteration 52000. Test Loss 0.690874457359314. Test Accuracy 53.43618392944336.\n",
      "Iteration 52500. Test Loss 0.6908157467842102. Test Accuracy 53.43618392944336.\n",
      "Iteration 53000. Test Loss 0.6907918453216553. Test Accuracy 53.43618392944336.\n",
      "Iteration 53500. Test Loss 0.6908501982688904. Test Accuracy 53.43618392944336.\n",
      "Iteration 54000. Test Loss 0.6909188628196716. Test Accuracy 53.43618392944336.\n",
      "Iteration 54500. Test Loss 0.691058337688446. Test Accuracy 53.43618392944336.\n",
      "Iteration 55000. Test Loss 0.6917137503623962. Test Accuracy 53.43618392944336.\n",
      "Iteration 55500. Test Loss 0.6924504637718201. Test Accuracy 53.43618392944336.\n",
      "Iteration 56000. Test Loss 0.6920682191848755. Test Accuracy 53.43618392944336.\n",
      "Iteration 56500. Test Loss 0.6914129853248596. Test Accuracy 53.43618392944336.\n",
      "Iteration 57000. Test Loss 0.6911172866821289. Test Accuracy 53.43618392944336.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(\n",
    "    train_text_data, \n",
    "    test_text_data, \n",
    "    nll_criterion, \n",
    "    num_epochs, \n",
    "    model, \n",
    "    optimiser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "20 mins"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
