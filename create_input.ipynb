{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch import nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title = pd.read_pickle('./data/clean_title.pkl')\n",
    "df_text = pd.read_pickle('./data/clean_text.pkl')\n",
    "print(df_title.columns)\n",
    "print(df_text.columns)\n",
    "print(df_title.head())\n",
    "print(df_text.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = Word2Vec(df_text[\"text\"], vector_size=100)\n",
    "model_title = Word2Vec(df_title[\"title\"], vector_size=100)\n",
    "model_text.save(\"./data/model_text\")\n",
    "model_title.save(\"./data/model_title\")\n",
    "print(model_title.corpus_total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text = Word2Vec.load(\"./data/model_text\")\n",
    "model_title = Word2Vec.load(\"./data/model_title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713\n",
      "torch.Size([713, 12202, 100])\n",
      "torch.Size([713])\n"
     ]
    }
   ],
   "source": [
    "# test with smaller amount of data\n",
    "FRACTION = 0.01\n",
    "size = int(df_text.shape[0] * FRACTION)\n",
    "print(size)\n",
    "\n",
    "\n",
    "# make tensors\n",
    "def text_to_tensor(text_list, word_model):\n",
    "    vectors = [word_model.wv[word] for word in text_list if word in word_model.wv]\n",
    "    return torch.tensor(vectors)\n",
    "\n",
    "\n",
    "def stack_tensor(df: pd.DataFrame, col_name, word_model):\n",
    "    tensor_ls = []\n",
    "    label_ls = []\n",
    "    for row in df.itertuples(index=False):\n",
    "        # print(row[df.columns.get_loc('class')])\n",
    "        # print(row[df.columns.get_loc(col_name)])\n",
    "        label_ls.append(row[df.columns.get_loc(\"class\")])\n",
    "        tensor_ls.append(text_to_tensor(row[df.columns.get_loc(col_name)], word_model))\n",
    "    padded_ls = pad_sequence(tensor_ls, batch_first=True)\n",
    "    return torch.tensor(label_ls), torch.stack(tuple(padded_ls))\n",
    "\n",
    "\n",
    "text_labels_tensor, text_tensor = stack_tensor(\n",
    "    df_text.iloc[:size, :], \n",
    "    col_name=\"text\", \n",
    "    word_model=model_text\n",
    ")\n",
    "print(text_tensor.size())\n",
    "print(text_labels_tensor.size())\n",
    "text_dataset = TensorDataset(text_tensor, text_labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "571\n"
     ]
    }
   ],
   "source": [
    "def split(tensor_dataset, test_split=0.2):\n",
    "    test_size = int(len(tensor_dataset) * test_split)\n",
    "    train_size = int(len(tensor_dataset) - test_size)\n",
    "    print(test_size)\n",
    "    print(train_size)\n",
    "    train_data, test_data = random_split(tensor_dataset, [train_size, test_size])\n",
    "    return train_data, test_data\n",
    "\n",
    "train_text_tensor, test_text_tensor = split(text_dataset,0.2)\n",
    "\n",
    "with open(\"./data/train_text\",\"wb\") as f:\n",
    "    pickle.dump(train_text_tensor,f)\n",
    "with open(\"./data/test_text\",\"wb\") as f:\n",
    "    pickle.dump(train_text_tensor,f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel should be 100, and the conv1d layer can be chosen because order of elements is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, dropout, activation_fn):\n",
    "        super(CNN, self).__init__()\n",
    "        self.hidden_layers = nn.ModuleList([])\n",
    "        self.hidden_layers.append(nn.Linear(input_size, 512))  # layer1\n",
    "        self.hidden_layers.append(nn.Linear(512, 256))  # layer2\n",
    "        self.hidden_layers.append(nn.Linear(256, 128))  # layer3\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.output_projection = nn.Linear(128, num_classes)\n",
    "        self.nonlinearity = activation_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            x = hidden_layer(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.nonlinearity(x)\n",
    "\n",
    "        out = self.output_projection(x)\n",
    "\n",
    "        out_distribution = F.log_softmax(out, dim=-1)\n",
    "        return out_distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
