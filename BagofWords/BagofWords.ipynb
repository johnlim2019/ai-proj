{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJnPRMzviivZ",
        "outputId": "6fd4d203-724c-4b2e-b5ec-92caa49b18d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting readability\n",
            "  Downloading readability-0.3.1.tar.gz (34 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: readability\n",
            "  Building wheel for readability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for readability: filename=readability-0.3.1-py3-none-any.whl size=35460 sha256=523f0e9cd5695b5a9244dd0ed7d2030eb039c856117e1d9d2d93a7eeaccc7db3\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/07/4d/2e3a0aaba1713619a403e1a3c56e88a6fc12d753872b98771c\n",
            "Successfully built readability\n",
            "Installing collected packages: readability\n",
            "Successfully installed readability-0.3.1\n",
            "Name: readability\n",
            "Version: 0.3.1\n",
            "Summary: Measure the readability of a given text using surface characteristics\n",
            "Home-page: https://github.com/andreasvc/readability/\n",
            "Author: Andreas van Cranenburgh\n",
            "Author-email: A.W.vanCranenburgh@uva.nl\n",
            "License: \n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: \n",
            "Required-by: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install readability\n",
        "!pip show readability\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn\n",
        "import readability\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "060OV4aVXGWt",
        "outputId": "74f24122-97f4-48ae-8b8d-f458260ee94d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWNQiKlWkgS9",
        "outputId": "95779961-0fba-4aff-ff62-acc82899b67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n",
            "(72134, 4)\n",
            "<class 'str'>\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/data/WELFake_Dataset.csv\")\n",
        "print(df.columns)\n",
        "print(df.shape)\n",
        "print(type(df.loc[7,\"text\"]))\n",
        "\n",
        "text_iterator = df['text']\n",
        "title_iterator = df['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqIIVaa-cqst",
        "outputId": "227ce2f8-34ba-43b9-f274-dea57f715d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "72134\n",
            "72134\n",
            "Now, most of the demonstrators gathered last night were exercising their constitutional and protected right to peaceful protest in order to raise issues and create change. Loretta Lynch aka Eric Holder in a skirt\n"
          ]
        }
      ],
      "source": [
        "# no stemming and removing stop words\n",
        "def process_text3(text_iterator):\n",
        "    text_processed = []\n",
        "    for text in text_iterator:\n",
        "        try:\n",
        "            # remove punctuation\n",
        "            t = re.sub(\"[^a-zA-Z0-9_.,!\\\"\\'/$]\", \" \", text)\n",
        "            # remove multiple spaces\n",
        "            t = re.sub(r\" +\", \" \", t)\n",
        "            # remove newline\n",
        "            t = re.sub(r\"\\n\", \" \", t)\n",
        "            # clear trailing whitespaces\n",
        "            t = t.strip()\n",
        "            # tokenise\n",
        "            t = t.split(\" \")\n",
        "            # drop empty string\n",
        "            t = list(filter(lambda x: x != \"\", t))\n",
        "            # join back the string\n",
        "            new_t = []\n",
        "            for w in t:\n",
        "              new_t.append(w)\n",
        "            # rejoin tokenized string\n",
        "            t = \" \".join(new_t)\n",
        "            if len(t) == 0:\n",
        "                text_processed.append(\"\")\n",
        "            else:\n",
        "                text_processed.append(t)\n",
        "        except:\n",
        "            text_processed.append(\"\")\n",
        "            continue\n",
        "    return text_processed\n",
        "\n",
        "\n",
        "text_processed = process_text3(text_iterator)\n",
        "title_processed = process_text3(title_iterator)\n",
        "\n",
        "print(len(text_processed))\n",
        "print(len(title_processed))\n",
        "print(text_processed[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pRFkYY5U_n2",
        "outputId": "ee2d7f53-f41b-469f-da01-3660d12bf7e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNBELIEVABLE! OBAMA S ATTORNEY GENERAL SAYS MOST CHARLOTTE RIOTERS WERE PEACEFUL PROTESTERS In Her Home State Of North Carolina VIDEO Now, most of the demonstrators gathered last night were exercising their constitutional and protected right to peaceful protest in order to raise issues and create change. Loretta Lynch aka Eric Holder in a skirt\n"
          ]
        }
      ],
      "source": [
        "merged_title_text = []\n",
        "for i,v in enumerate(title_processed):\n",
        "    if (v == None):\n",
        "        merge = text_processed[i]\n",
        "    elif (text_processed[i]==None):\n",
        "        merge = v\n",
        "    else:\n",
        "        merge = v + \" \" + text_processed[i]\n",
        "    merged_title_text.append(merge)\n",
        "\n",
        "print(merged_title_text[2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10ejZniS3NNa"
      },
      "outputs": [],
      "source": [
        "df_output = pd.DataFrame(columns=[\"title\", \"text\", \"merged\", \"class\"])\n",
        "df_output[\"title\"] = title_processed\n",
        "df_output[\"text\"] = text_processed\n",
        "df_output[\"merged\"] = merged_title_text\n",
        "df_output[\"class\"] = df[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xE3ryPL_3qe6",
        "outputId": "6accfa89-b77b-42bd-fafc-cb24f7a10e68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60264    Elon Musk s Tesla Stock Up $2 Billion Since Jo...\n",
            "42050    Wharton Business School Backers Seek Distance ...\n",
            "62289    Border Patrol Agents Arrest Smuggler After Rol...\n",
            "Name: merged, dtype: object\n",
            "60264    0\n",
            "42050    0\n",
            "62289    0\n",
            "Name: class, dtype: int64\n",
            "57707\n",
            "On Trump s victory Gilad Atzmon It occurred to me in recent years that the act of being progressive is not a political position but rather a mental state. The incapacity of the entire American progressive and left establishment to foresee Trump s landslide victory suggests that we are dealing with people who are institutionally detached Just three days ahead of the presidential elections, the Huffington Post pathetically criticised star pollster Nate Silver of Unskewing Polls in Trump s direction, for suggesting that a Trump victory was realistic. Ryan Grim wrote HuffPost Pollster is giving Clinton a 98 percent chance of winning, and The New York Times model at The Upshot puts her chances at 85 percent. There is one outlier, however, that is causing waves of panic among Democrats around the country and injecting Trump backers with the hope that their guy might pull this thing off after all. Nate Silver s 538 model gives Donald Trump a heart stopping 35 chance of winning as of this weekend. The Huffington Post went as far as accusing Silver of making a mockery of the very forecasting industry that he popularized. In perspectives, Nate Silver and his 538 were obviously spot on. The Huffington Post and The New York Times were totally off the mark. Is it a coincidence How is it possible that the Democratic Party, the mainstream media and Wall Street have managed to totally miss the level of anger that unites the American masses. These questions go far beyond polling strategy or the science of statistics. We are dealing with a state of being aloof on the verge of total detachment. Left and progressive thinking is shaped like a dream. It tells us what the world ought to be. Progressives often seem to forget what the world really is and what its people are really like. Hillary Clinton and her campaign, just like the New York Times and The Huffington Post, were in a state of denial. Boasting in righteous hubris, they failed to read the map. But this shouldn t take us by a complete surprise. Detachment wasn t invented by Clinton and her team. Detachment and alienation are ingrained in progressive thought. To be a progressive is to believe that some of the other people are simply a bunch of unaware reactionaries. Progressive thought is the secular manifestation of chosenness. It is inherently Jewish, a fact that explains why Hillary Clinton s top five donors were Jewish billionaires . Since being progressive is a form of supremacy. I would go as far as suggesting that progressives antagonism towards white supremacy, is at large, a form of projection. The progressive attributes to whiteness his own exceptionalist inclinations. Americans vs. Identitarians On election day, we learned that the Democratic Party was hanging on a thread, hoping to be saved by Florida s Hispanic vote. Clinton s political future depended upon the hope that Trump had managed to upset enough Latinos. This peculiar development in which a national party is dependent on group politics shouldn t take us by surprise anymore. The 2016 American presidential election divided America into two camps The Americans on one side and the Identitarians on the other. The Americans are those who see themselves primarily as American patriots. They are driven by rootedness and heritage. For them, the promise to make America great again confirms that Utopia is nostalgia and that the progressive reality is nothing short of dystopia. The Identitarians, on the other hand, are those who subscribe to progressive sectarian politics. They see themselves primarily as LGBTQ, Latino, Black, Jews, Women, and so on. Their bond with the American national or patriotic ethos is secondary and often non existent. The future of the Democratic Party, in its current form, depends upon the hope that American subscriptions to sectarian ideologies will gradually increase and, as a result, will eventually strengthen the context of identity or group politics. The progressive agenda banks on the divestiture of the national and patriotic ethos. Needless to mention that half of America voted for Clinton. Hence, this political agenda is far from being farfetched or delusional. But the Identitarian agenda backfired. It was only a question of time before the so called whites or rednecks grasped that their backs have been pressed to the wall. They also started to act and think as an identitarian political sector. Hillary Clinton calling Trump s voters a basket of deplorables was a clear sign for white poor Americans that Hillary wasn t exactly their ally. However, Hillary was far from being alone. Almost every Jewish writer within the American press didn t miss the opportunity to attribute the White Supremacist label to Trump s voters. For Cheryl Greenberg, Trump s popularity was the final gasping of white supremacy. For T alking Points Memo s Josh Marshall , Trump s closing ad was packed with anti Semitic dog whistles, anti Semitic tropes, and anti Semitic vocabulary. For Marshall and Goldberg, half of the American people were dogs obeying their master s whistle. It shouldn t take us by surprise that half of the American people would eventually react. They became weary of Jewish progressives like Marshall and Goldberg seeing them as dogs and white supremacists. The time was ripe for a revolution. So is the revolution here I m not holding my breath. The people who crowned Trump are certainly exhausted. They are ready for a change. Can Trump introduce such a change No one knows. He is certainly going to keep us entertained. Gilad Atzmon is an Israeli jazz musician, author and political activist. His new book, The Wandering Who, may be ordered from amazon.com or amazon.co.uk .\n"
          ]
        }
      ],
      "source": [
        "# split test and training dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(df_output[\"merged\"], df_output[\"class\"], test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "print(x_train[0:3])\n",
        "print(y_train[0:3])\n",
        "print(len(x_train))\n",
        "print(x_train[57705])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5cCKPs24bZy"
      },
      "outputs": [],
      "source": [
        "# get text length\n",
        "def get_text_length(x):\n",
        "    return np.array([len(t) for t in x]).reshape(-1, 1)\n",
        "\n",
        "# print((get_text_length(x_test[5:15])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es6hvXuJXk8m"
      },
      "outputs": [],
      "source": [
        "# get pos of words\n",
        "stopWords = stopwords.words('english')\n",
        "def get_pos_words(x):\n",
        "  n_ctr, a_ctr, v_ctr, r_ctr, others, stopwords = 0, 0, 0, 0, 0, 0\n",
        "  classified = \"\"\n",
        "  counters = []\n",
        "  output = []\n",
        "  for t in x:\n",
        "    counters = []\n",
        "    n_ctr, a_ctr, v_ctr, r_ctr, others, stopwords = 0, 0, 0, 0, 0, 0\n",
        "    for w in t.split(\" \"):\n",
        "      if w not in stopWords:\n",
        "        if wn.synsets(w):\n",
        "          classified = wn.synsets(w)[0].pos()\n",
        "          if classified == \"n\": n_ctr += 1\n",
        "          elif classified == \"a\": a_ctr += 1\n",
        "          elif classified == \"v\": v_ctr += 1\n",
        "          elif classified == \"r\": r_ctr += 1\n",
        "        else: others += 1\n",
        "      else: stopwords += 1\n",
        "    counters.append(n_ctr/len(t.split(\" \")))\n",
        "    counters.append(a_ctr/len(t.split(\" \")))\n",
        "    counters.append(v_ctr/len(t.split(\" \")))\n",
        "    counters.append(r_ctr/len(t.split(\" \")))\n",
        "    counters.append(others/len(t.split(\" \")))\n",
        "    counters.append(stopwords/len(t.split(\" \")))\n",
        "    output.append(counters)\n",
        "  return np.array(output)\n",
        "\n",
        "# print(get_pos_words(x_test[5:15]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmH7uvMzmk6k"
      },
      "outputs": [],
      "source": [
        "# get no. of capital letters\n",
        "def get_capital_letters(x):\n",
        "  counter = 0\n",
        "  total = []\n",
        "  for t in x:\n",
        "    number = []\n",
        "    counter = 0\n",
        "    for l in t:\n",
        "      if l.isupper(): counter+=1\n",
        "    number.append(counter/len(t))\n",
        "    total.append(number)\n",
        "  return np.array(total)\n",
        "\n",
        "# print(get_capital_letters(x_test[5:10]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get no. of short/long sentences\n",
        "# the number of words that determine whether a sentence is long (30), gotten from\n",
        "# https://languagetool.org/insights/post/sentence-length/#:~:text=Sentences%20are%20usually%20between%2015,should%20be%20considered%20the%20maximum.\n",
        "def get_long_short_sentences(x):\n",
        "  longg, short = 0, 0\n",
        "  total = []\n",
        "  for t in x:\n",
        "    longg, short = 0, 0\n",
        "    count = []\n",
        "    sentence_list = nltk.sent_tokenize(t)\n",
        "    for s in sentence_list:\n",
        "      no_of_words = s.split(\" \")\n",
        "      if len(no_of_words) > 30:\n",
        "        longg += 1\n",
        "      else:\n",
        "        short += 1\n",
        "    if len(sentence_list) != 0:\n",
        "      count.append(longg/len(sentence_list))\n",
        "      count.append(short/len(sentence_list))\n",
        "    else:\n",
        "      count.append(0)\n",
        "      count.append(0)\n",
        "    total.append(count)\n",
        "  return np.array(total)\n",
        "\n",
        "\n",
        "# print(get_long_short_sentences(x_train).shape)"
      ],
      "metadata": {
        "id": "cD10qLdv7a9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get no. of special characters\n",
        "# what is considered a special character taken from\n",
        "# https://www.webopedia.com/definitions/special-character/#:~:text=A%20special%20character%20is%20one,marks%20are%20also%20special%20characters.\n",
        "def get_special_characters(x):\n",
        "  total = []\n",
        "  for t in x:\n",
        "    characters = re.sub('[\\w]+','',t)\n",
        "    characters = re.sub(r\" +\", \"\", characters)\n",
        "    if len(t) != 0:\n",
        "      total.append([len(characters)/len(t)])\n",
        "    else:\n",
        "      total.append([0])\n",
        "  return np.array(total)\n",
        "\n",
        "# print(get_special_characters(x_test[5:10]))"
      ],
      "metadata": {
        "id": "SdU_b9nEBHaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_readability(x):\n",
        "  indexes = []\n",
        "  result = []\n",
        "  for t in x:\n",
        "    indexes = []\n",
        "    try:\n",
        "        r = readability.getmeasures(t)\n",
        "        indexes.append(r['readability grades']['DaleChallIndex'] if r['readability grades']['DaleChallIndex'] > 0 else 0)\n",
        "        indexes.append(r['readability grades']['GunningFogIndex'] if r['readability grades']['GunningFogIndex'] > 0 else 0)\n",
        "        indexes.append(r['readability grades']['SMOGIndex'] if r['readability grades']['SMOGIndex'] > 0 else 0)\n",
        "        indexes.append(r['readability grades']['FleschReadingEase'] if r['readability grades']['FleschReadingEase'] > 0 else 0)\n",
        "        indexes.append(r['readability grades']['ARI'] if r['readability grades']['ARI'] > 0 else 0)\n",
        "    except:\n",
        "        r = 0\n",
        "        indexes = [0, 0, 0, 0, 0]\n",
        "    result.append(indexes)\n",
        "  return np.array(result)\n",
        "\n",
        "# print(get_readability(x_test[5:10]))"
      ],
      "metadata": {
        "id": "NDxlX7gn5CkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy(predicted, test):\n",
        "  num_correct = 0\n",
        "  for i in range(len(test)):\n",
        "    if predicted[i] == test[i]:\n",
        "      num_correct +=  1\n",
        "  return num_correct/len(test)"
      ],
      "metadata": {
        "id": "-21gPAVU_kxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waiHEyknBxYu",
        "outputId": "b29b30d9-f55f-40cd-e448-06f5ece7e07b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8674014001524919\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model\n",
        "classifier = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(min_df=0, max_df=1.0)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())])\n",
        "classifier.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted = classifier.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normal bag of words model\n",
        "classifier = Pipeline([\n",
        "    ('vectorizer', CountVectorizer(min_df=0, max_df=1.0, max_features=500)),\n",
        "    ('tfidf', TfidfTransformer()),\n",
        "    ('clf', MultinomialNB())])\n",
        "classifier.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted = classifier.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted, np.array(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BWdvUvzhs6q",
        "outputId": "f0a6c104-e31e-4dc2-c5f1-394524281d1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8423095584667637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lN6OP4Az4frx",
        "outputId": "03072294-1a3a-4d57-91a8-53442b789a85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8597074928952658\n"
          ]
        }
      ],
      "source": [
        "# bag of words model with text length\n",
        "classifier2 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=1.0)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('length', Pipeline([\n",
        "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier2.fit(x_train, y_train.ravel())\n",
        "predicted2 = classifier2.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted2, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92ddf55b-0bb5-473f-e262-29b8661b5e36",
        "id": "-Ss5dkk2lshD"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8676786580716712\n"
          ]
        }
      ],
      "source": [
        "# bag of words model with text length\n",
        "classifier2 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=1.0)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('length', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier2.fit(x_train, y_train.ravel())\n",
        "predicted2 = classifier2.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted2, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsAcNrFYdj08",
        "outputId": "6a2a0b27-11ef-4324-eae4-2d9da85a1eb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8616482983295211\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model with length and pos\n",
        "classifier3 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.95)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('length', Pipeline([\n",
        "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier3.fit(x_train, y_train.ravel())\n",
        "predicted3 = classifier3.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted3, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMdjMcN-dpSi",
        "outputId": "259cb679-3d8a-4a76-bf33-dcc31990777a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8610937824911624\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model with length and pos and proportion of capital letters\n",
        "classifier4 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.95)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('length', Pipeline([\n",
        "            ('count', FunctionTransformer(get_text_length, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier4.fit(x_train, y_train.ravel())\n",
        "predicted4 = classifier4.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted4, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuYeCRVvItbZ",
        "outputId": "10a219a4-886d-4ce5-f49d-b4f1ee288aa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8692728911069523\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters\n",
        "classifier5 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.95)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier5.fit(x_train, y_train.ravel())\n",
        "predicted5 = classifier5.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted5, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3ob-CXlohcn",
        "outputId": "cdea0992-447e-461b-d3e2-dd7e41d69f19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8699660359049005\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier6 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.85)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier6.fit(x_train, y_train.ravel())\n",
        "predicted6 = classifier6.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted6, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normal bag of words model and proportion of capital letters, pos and readability\n",
        "classifier11 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.85)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('readability', Pipeline([\n",
        "            ('count', FunctionTransformer(get_readability, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier11.fit(x_train, y_train.ravel())\n",
        "predicted11 = classifier11.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted11, np.array(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJl3W9C_-Rab",
        "outputId": "ea69b970-ac48-45e2-ba9f-f6e44f9c9382"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.58418243571082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eh8XNSzmojTN",
        "outputId": "3142f6fa-8827-488f-b3f1-e34c88cbb6bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8699660359049005\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier7 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.75)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier7.fit(x_train, y_train.ravel())\n",
        "predicted7 = classifier6.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted7, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFb3L2AVyMqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d5b8ff-8dca-414e-fae6-643355a3b811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8696887779857212\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier8 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.95)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier8.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted8 = classifier8.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted8, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5SQC0CcZDtyL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c102e9f-77ac-4bec-e61a-182ce3354715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.869758092465516\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier9 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.95)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('sc', Pipeline([\n",
        "            ('count', FunctionTransformer(get_special_characters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier9.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted9 = classifier9.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted9, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYlk7YXecc-h",
        "outputId": "ef851f1a-7313-4125-a428-4b441191eb5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8698967214251057\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier10 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.75)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier10.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted10 = classifier10.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted10, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normal bag of words model and proportion of capital letters, short long sentences and pos\n",
        "classifier10 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.75)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier10.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted10 = classifier10.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted10, np.array(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k-LD_FQniq9",
        "outputId": "5aad422e-93f1-4496-a176-0ae96780639b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8703126083038747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# normal bag of words model and proportion of capital letters, short long sentences and pos\n",
        "classifier10 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.65)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier10.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted10 = classifier10.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted10, np.array(y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39efcbf3-82f0-40b5-ec53-e0c65bcb9fcf",
        "id": "cOYS-CmOr4JO"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8683024883898246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a57799-11cd-4ad3-d8f2-46fd16a5fa87",
        "id": "a4uoG2ADo4J8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8698274069453109\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier9 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.85)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('sc', Pipeline([\n",
        "            ('count', FunctionTransformer(get_special_characters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier9.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted9 = classifier9.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted9, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc1c40b9-d621-4286-cd48-9bd4d14e4580",
        "id": "GZVeY1dVo6F2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8703819227836695\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier9 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.75)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('sc', Pipeline([\n",
        "            ('count', FunctionTransformer(get_special_characters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier9.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted9 = classifier9.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted9, np.array(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b49ea2-6d68-4154-96e8-55d8d0aed199",
        "id": "BpKNtw-92jYF"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8683024883898246\n"
          ]
        }
      ],
      "source": [
        "# normal bag of words model and proportion of capital letters and pos\n",
        "classifier9 = Pipeline([\n",
        "    ('features', FeatureUnion([\n",
        "        ('text', Pipeline([\n",
        "            ('vectorizer', CountVectorizer(min_df=0,max_df=0.65)),\n",
        "            ('tfidf', TfidfTransformer()),\n",
        "        ])),\n",
        "        ('capital', Pipeline([\n",
        "            ('count', FunctionTransformer(get_capital_letters, validate=False)),\n",
        "        ])),\n",
        "        ('pos', Pipeline([\n",
        "            ('count', FunctionTransformer(get_pos_words, validate=False)),\n",
        "        ])),\n",
        "        ('sls', Pipeline([\n",
        "            ('count', FunctionTransformer(get_long_short_sentences, validate=False)),\n",
        "        ])),\n",
        "        ('sc', Pipeline([\n",
        "            ('count', FunctionTransformer(get_special_characters, validate=False)),\n",
        "        ]))\n",
        "    ])),\n",
        "    ('clf', MultinomialNB())])\n",
        "\n",
        "classifier9.fit(x_train, y_train.ravel())\n",
        "\n",
        "predicted9 = classifier9.predict(x_test)\n",
        "\n",
        "print(get_accuracy(predicted9, np.array(y_test)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}