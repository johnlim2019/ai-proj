{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'title', 'text', 'label'], dtype='object')\n",
      "(72134, 4)\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"./data/WELFake_Dataset.csv\")\n",
    "print(df.columns)\n",
    "print(df.shape)\n",
    "print(type(df.loc[7,\"text\"]))\n",
    "\n",
    "text_iterator = df['text']\n",
    "title_iterator = df['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72134\n",
      "72134\n",
      "['now', 'most', 'of', 'the', 'demonstrators', 'gathered', 'last', 'night', 'were', 'exercising', 'their', 'constitutional', 'and', 'protected', 'right', 'to', 'peaceful', 'protest', 'in', 'order', 'to', 'raise', 'issues', 'and', 'create', 'change', 'loretta', 'lynch', 'aka', 'eric', 'holder', 'in', 'a', 'skirt']\n"
     ]
    }
   ],
   "source": [
    "def process_text(text_iterator):\n",
    "    text_processed = []\n",
    "    for text in text_iterator:        \n",
    "        try:\n",
    "            # remove punctuation\n",
    "            t = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "            # remove multiple spaces\n",
    "            t = re.sub(r\" +\", \" \", t)\n",
    "            # remove newline\n",
    "            t = re.sub(r\"\\n\", \" \", t)\n",
    "            # clear trailing whitespaces\n",
    "            t = t.strip()\n",
    "            # lowercase\n",
    "            t = t.lower()\n",
    "            # tokenise\n",
    "            t = t.split(\" \")\n",
    "            # drop empty string\n",
    "            t = list(filter(lambda x: x != \"\", t))\n",
    "            if len(t) == 0:\n",
    "                text_processed.append(None)\n",
    "            else:\n",
    "                text_processed.append(t)\n",
    "        except:\n",
    "            text_processed.append(None)\n",
    "            continue\n",
    "    return text_processed\n",
    "\n",
    "\n",
    "text_processed = process_text(text_iterator)\n",
    "title_processed = process_text(title_iterator)\n",
    "\n",
    "print(len(text_processed))\n",
    "print(len(title_processed))\n",
    "print(text_processed[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/limjohn/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['demonstrators', 'gathered', 'last', 'night', 'exercising', 'constitutional', 'protected', 'right', 'peaceful', 'protest', 'order', 'raise', 'issues', 'create', 'change', 'loretta', 'lynch', 'aka', 'eric', 'holder', 'skirt']\n",
      "72134\n",
      "72134\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stop_words_inner(text, stop_words):\n",
    "    no_stop_words = []\n",
    "    if text is None:\n",
    "        return None\n",
    "    for token in text:\n",
    "        if token not in stop_words:\n",
    "            no_stop_words.append(token)\n",
    "    return no_stop_words\n",
    "\n",
    "\n",
    "def remove_stop_words_outer(text_list, stop_words):\n",
    "    for index, text in enumerate(text_list):\n",
    "        t = remove_stop_words_inner(text, stop_words)\n",
    "        text_list[index] = t\n",
    "    return text_list\n",
    "\n",
    "\n",
    "text_no_stop_words = remove_stop_words_outer(text_processed, stop_words)\n",
    "title_no_stop_words = remove_stop_words_outer(title_processed, stop_words)\n",
    "print(text_no_stop_words[2])\n",
    "print(len(text_no_stop_words))\n",
    "print(len(title_no_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72134\n"
     ]
    }
   ],
   "source": [
    "merged_title_text = []\n",
    "for i,v in enumerate(title_no_stop_words):\n",
    "    if (v == None):    \n",
    "        merge = text_no_stop_words[i]\n",
    "    elif (text_no_stop_words[i]==None):\n",
    "        merge = v\n",
    "    else:\n",
    "        merge = v + text_no_stop_words[i]\n",
    "    merged_title_text.append(merge)\n",
    "    \n",
    "print(len(merged_title_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new table\n",
    "\n",
    "We will keep the stop words in the text, as our intended use was Word2Vec, which is sequential learning and stop words affect sequential learning. \n",
    "\n",
    "If we are using content features that are not affected by sequence, then removing stop words is necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = pd.DataFrame(columns=[\"index\", \"tokens\", \"class\"])\n",
    "df_output[\"tokens\"] = merged_title_text\n",
    "df_output[\"index\"] = np.arange(len(merged_title_text))\n",
    "df_output[\"class\"] = df[\"label\"]\n",
    "df_output.to_csv(\"./data/clean_data_merged_title_text.csv\")\n",
    "df_output.to_pickle(\"./data/clean_title_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
